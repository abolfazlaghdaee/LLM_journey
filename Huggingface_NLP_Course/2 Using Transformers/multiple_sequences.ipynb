{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPvwZK32u2e6gsg7AK7TFfQ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/abolfazlaghdaee/AI_Project/blob/main/multiple_sequences.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the previous section, we explored the simplest of use cases: doing inference on a single sequence of a small length. However, some questions emerge already:\n",
        "\n",
        "- How do we handle multiple sequences?\n",
        "- How do we handle multiple sequences of different lengths?\n",
        "- Are vocabulary indices the only inputs that allow a model to work well?\n",
        "- Is there such a thing as too long a sequence?\n",
        "\n",
        "\n",
        "Let’s see what kinds of problems these questions pose, and how we can solve them using the 🤗 Transformers API."
      ],
      "metadata": {
        "id": "XyNh7lAFXHzu"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XbhlRKqtXAj8",
        "outputId": "01d03fb5-de15-41ca-f183-29b7e8ddebb8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.48.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.17.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.28.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.0)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (2024.10.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.1.31)\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Models expect a batch of inputs**"
      ],
      "metadata": {
        "id": "Zi1q-ii_XYrm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from transformers import AutoTokenizer, TFAutoModelForSequenceClassification\n",
        "\n",
        "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
        "\n",
        "model = TFAutoModelForSequenceClassification.from_pretrained(checkpoint)\n",
        "\n",
        "sequence = \"I've been waiting for a HuggingFace course my whole life.\"\n",
        "\n",
        "\n",
        "tokens = tokenizer.tokenize(sequence)\n",
        "\n",
        "ids = tokenizer.convert_tokens_to_ids(tokens)\n",
        "\n",
        "\n",
        "input_ids = tf.constant(ids)\n",
        "\n",
        "output = model(input_ids)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gRW36beCXVUN",
        "outputId": "c32297ec-61b8-4f4e-f9a2-0991200daba5"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "All PyTorch model weights were used when initializing TFDistilBertForSequenceClassification.\n",
            "\n",
            "All the weights of TFDistilBertForSequenceClassification were initialized from the PyTorch model.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertForSequenceClassification for predictions without further training.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "output.logits"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c2WTHdf4brLB",
        "outputId": "7c4e6f7c-99ea-4913-d2a1-29b97747e376"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(1, 2), dtype=float32, numpy=array([[-2.7276208,  2.8789372]], dtype=float32)>"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Oh no! Why did this fail? We followed the steps from the pipeline in section 2.\n",
        "\n",
        "The problem is that we sent a single sequence to the model, whereas 🤗 Transformers models expect multiple sentences by default. Here we tried to do everything the tokenizer did behind the scenes when we applied it to a sequence. But if you look closely, you’ll see that the tokenizer didn’t just convert the list of input IDs into a tensor, it added a dimension on top of it:"
      ],
      "metadata": {
        "id": "Vi8EYwffacDr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_inputs = tokenizer(sequence, return_tensors=\"tf\")\n",
        "\n",
        "print(tokenized_inputs[\"input_ids\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TqhjHPH8aOzw",
        "outputId": "9fbf1459-22c6-4cd4-f766-ba5ddc2bbf80"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(\n",
            "[[  101  1045  1005  2310  2042  3403  2005  1037 17662 12172  2607  2026\n",
            "   2878  2166  1012   102]], shape=(1, 16), dtype=int32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Padding the inputs**\n",
        "\n",
        "In order to work around this, we’ll use `padding` to make our tensors have a rectangular shape. Padding makes sure all our sentences have the same length by adding a special word called the padding token to the sentences with fewer values. For example, if you have 10 sentences with 10 words and 1 sentence with 20 words, padding will ensure all the sentences have 20 words. In our example, the resulting tensor looks like this:"
      ],
      "metadata": {
        "id": "Mxnv6tIGeIx3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "padding_id = 100\n",
        "\n",
        "batched_ids = [\n",
        "    [200, 200, 200],\n",
        "    [200, 200, padding_id],\n",
        "]"
      ],
      "metadata": {
        "id": "e6B08fm4ammu"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model  = TFAutoModelForSequenceClassification.from_pretrained(checkpoint)\n",
        "\n",
        "\n",
        "sequence1_ids = [[200, 200, 200]]\n",
        "sequence2_ids = [[200, 200]]\n",
        "\n",
        "batched_ids = [\n",
        "    [200, 200, 200],\n",
        "    [200, 200, tokenizer.pad_token_id],\n",
        "]\n",
        "\n",
        "print(model(tf.constant(sequence1_ids)).logits)\n",
        "print(model(tf.constant(sequence2_ids)).logits)\n",
        "print(model(tf.constant(batched_ids)).logits)"
      ],
      "metadata": {
        "id": "s7qYGkxdeZ6y",
        "outputId": "a18a14fe-3ac1-40da-8ea1-a72cf5477164",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "All PyTorch model weights were used when initializing TFDistilBertForSequenceClassification.\n",
            "\n",
            "All the weights of TFDistilBertForSequenceClassification were initialized from the PyTorch model.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertForSequenceClassification for predictions without further training.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor([[ 1.5693678 -1.3894583]], shape=(1, 2), dtype=float32)\n",
            "tf.Tensor([[ 0.58030033 -0.41252464]], shape=(1, 2), dtype=float32)\n",
            "tf.Tensor(\n",
            "[[ 1.569368  -1.3894584]\n",
            " [ 1.3373476 -1.2163184]], shape=(2, 2), dtype=float32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Attention masks**\n",
        "\n",
        "Attention masks are tensors with the exact same shape as the input IDs tensor, filled with 0s and 1s: 1s indicate the corresponding tokens should be attended to, and 0s indicate the corresponding tokens should not be attended to (i.e., they should be ignored by the attention layers of the model).\n",
        "\n",
        "Let’s complete the previous example with an attention mask:"
      ],
      "metadata": {
        "id": "kr_u4jFMfF8W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batched_ids = [\n",
        "    [200, 200, 200],\n",
        "    [200, 200, tokenizer.pad_token_id],\n",
        "]\n",
        "\n",
        "attention_mask = [\n",
        "    [1, 1, 1],\n",
        "    [1, 1, 0],\n",
        "]\n",
        "\n",
        "outputs = model(tf.constant(batched_ids), attention_mask = tf.constant(attention_mask))\n",
        "\n",
        "\n",
        "outputs.logits"
      ],
      "metadata": {
        "id": "2kQYhDVmeu33",
        "outputId": "c4fda19c-de29-4ba4-a478-369c578b04c0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(2, 2), dtype=float32, numpy=\n",
              "array([[ 1.569368  , -1.3894584 ],\n",
              "       [ 0.58030206, -0.41252613]], dtype=float32)>"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "✏️ Try it out! Apply the tokenization manually on the two sentences used in section 2 (“I’ve been waiting for a HuggingFace course my whole life.” and “I hate this so much!”). Pass them through the model and check that you get the same logits as in section 2. Now batch them together using the padding token, then create the proper attention mask. Check that you obtain the same results when going through the model!"
      ],
      "metadata": {
        "id": "dbKYFN4zhRTE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sequence1 = \"I’ve been waiting for a HuggingFace course my whole life.\"\n",
        "sequence2 = \"I hate this so much!\"\n",
        "\n",
        "\n",
        "tokens1 = tokenizer.tokenize(sequence1)\n",
        "tokens2 = tokenizer.tokenize(sequence2)\n",
        "\n",
        "ids1 = tokenizer.convert_tokens_to_ids(tokens1)\n",
        "ids2 = tokenizer.convert_tokens_to_ids(tokens2)\n",
        "\n",
        "\n",
        "input_ids1 = tf.constant(ids1)\n",
        "input_ids2 = tf.constant(ids2)\n",
        "\n",
        "\n",
        "\n",
        "outputs1 = model(input_ids1)\n",
        "outputs2 = model(input_ids2)\n",
        "\n",
        "print(outputs1.logits)\n",
        "outputs2.logits\n"
      ],
      "metadata": {
        "id": "iCFSls5vftqQ",
        "outputId": "1da69e26-dbf2-4c07-8910-e83d15c598cb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor([[-2.571974   2.6852386]], shape=(1, 2), dtype=float32)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(1, 2), dtype=float32, numpy=array([[ 3.1930926, -2.6685236]], dtype=float32)>"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "max_length = max(len(ids1), len(ids2))\n",
        "\n",
        "\n",
        "ids_1_padded = ids1 + [tokenizer.pad_token_id] * (max_length - len(ids1))\n",
        "ids_2_padded = ids2 + [tokenizer.pad_token_id] * (max_length - len(ids2))\n",
        "\n",
        "# Create attention masks (1 for real tokens, 0 for padding tokens)\n",
        "attention_mask_1 = [1] * len(ids1) + [0] * (max_length - len(ids1))\n",
        "attention_mask_2 = [1] * len(ids2) + [0] * (max_length - len(ids2))\n",
        "\n",
        "\n",
        "batched_input_ids = tf.constant([ids_1_padded, ids_2_padded])\n",
        "batched_attention_mask = tf.constant([attention_mask_1, attention_mask_2])\n",
        "\n",
        "batched_output = model(batched_input_ids, attention_mask=batched_attention_mask).logits\n",
        "batched_output\n"
      ],
      "metadata": {
        "id": "CXwMUlwnimpC",
        "outputId": "82c3b7c7-7677-40cb-9875-ee4bbceae583",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(2, 2), dtype=float32, numpy=\n",
              "array([[-2.5719736,  2.6852374],\n",
              "       [ 3.1930914, -2.668523 ]], dtype=float32)>"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Longer sequences**\n",
        "\n",
        "With Transformer models, there is a limit to the lengths of the sequences we can pass the models. Most models handle sequences of up to 512 or 1024 tokens, and will crash when asked to process longer sequences. There are two solutions to this problem:\n",
        "\n",
        "- Use a model with a longer supported sequence length.\n",
        "- Truncate your sequences.\n",
        "\n",
        "\n",
        "Models have different supported sequence lengths, and some specialize in handling very long sequences. Longformer is one example, and another is LED. If you’re working on a task that requires very long sequences, we recommend you take a look at those models.\n",
        "\n",
        "Otherwise, we recommend you truncate your sequences by specifying the max_sequence_length parameter:\n"
      ],
      "metadata": {
        "id": "0by_hSzmldOv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sequence = sequence[:max_sequence_length]"
      ],
      "metadata": {
        "id": "Bz7-S7Y-l0Sd"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}