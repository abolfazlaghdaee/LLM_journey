{"cells":[{"cell_type":"markdown","id":"b057fad5","metadata":{"id":"b057fad5"},"source":["# **Practice Assignment: NLP with NLTK & spaCy**\n","\n","* This assignment is part of the NLP Workshop on YouTube, which is free and open to the public.\n","* **Lecturer: Reza Shokrzad.**\n","*‚Äå [ÿØÿ≥ÿ™ÿ±ÿ≥€å ÿ®Ÿá ÿ¨ŸÑÿ≥Ÿá ÿßŸàŸÑ ⁄©ŸÑÿßÿ≥](https://youtube.com/live/lDCoqQSc4ZE?feature=share)\n","* [ÿ®ÿ±ŸÜÿßŸÖŸá ÿßÿ¨ÿ±ÿß€å€å ⁄©ŸÑÿßÿ≥ Ÿà ÿ¨ŸÑÿ≥ÿßÿ™](https://docs.google.com/spreadsheets/d/1SP3NJ9H7yp8sgof-zp_t4oxmdxjMdEgoL_mmCDvdUm4/edit?gid=0#gid=0)\n","\n","\n","Welcome to this **Fill-in-the-Blanks NLP Assignment!** üéØ This exercise will help you solidify your understanding of **NLTK** and **spaCy** by filling in the missing parts of the code. Follow the instructions carefully, and make sure to test your solutions!\n"]},{"cell_type":"markdown","id":"aa085b7f","metadata":{"id":"aa085b7f"},"source":["## **1. Working with Corpora & Lexical Resources**\n","**Task:** Load and analyze texts from different corpora.\n","- Use NLTK‚Äôs **Gutenberg** corpus to load the text of *Moby Dick*.\n","- Tokenize it into words.\n","- Count the top 10 most frequent words (excluding stopwords)."]},{"cell_type":"code","source":["!pip install nltk"],"metadata":{"id":"p8W1Ulwv7Bxn","executionInfo":{"status":"ok","timestamp":1741792586190,"user_tz":-210,"elapsed":5364,"user":{"displayName":"abolfazl aghdaee","userId":"17465871458475218528"}},"outputId":"0f6b3a17-f112-435a-a60e-07cb8114e3a1","colab":{"base_uri":"https://localhost:8080/"}},"id":"p8W1Ulwv7Bxn","execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n","Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.1.8)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.4.2)\n","Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk) (4.67.1)\n"]}]},{"cell_type":"code","execution_count":4,"id":"757d9099","metadata":{"id":"757d9099","executionInfo":{"status":"ok","timestamp":1741792795576,"user_tz":-210,"elapsed":89358,"user":{"displayName":"abolfazl aghdaee","userId":"17465871458475218528"}},"outputId":"b5cba629-d830-4626-f5eb-25fcdf85f635","colab":{"base_uri":"https://localhost:8080/"}},"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package gutenberg to /root/nltk_data...\n","[nltk_data]   Package gutenberg is already up-to-date!\n","[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n","[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n","[nltk_data]   Package punkt_tab is already up-to-date!\n"]},{"output_type":"stream","name":"stdout","text":["[('i', 2121), ('ahab', 511), ('stubb', 257), ('queequeg', 252), ('starbuck', 198), ('pequod', 172), ('nantucket', 95), ('jonah', 85), ('dick', 83), ('moby', 82)]\n"]}],"source":["import nltk\n","nltk.download('gutenberg')\n","nltk.download('stopwords')\n","nltk.download('punkt_tab')\n","\n","from nltk.corpus import gutenberg\n","from nltk.probability import FreqDist\n","from nltk.corpus import stopwords\n","\n","\n","\n","# Load text\n","text = gutenberg.raw('melville-moby_dick.txt')  # FILL THIS\n","\n","# Tokenize words\n","words = nltk.word_tokenize(text)\n","\n","stop_words = set(stopwords.words('english'))\n","\n","# Remove stopwords\n","filtered_words =  [word.lower() for word in words if word.isalnum() and word.lower() not in words and stop_words ] # FILL THIS\n","\n","# Compute frequency distribution\n","fdist = FreqDist(filtered_words)\n","\n","# Print top 10 words\n","print(fdist.most_common(10))"]},{"cell_type":"markdown","id":"f2894d7a","metadata":{"id":"f2894d7a"},"source":["## **2. Tokenization Techniques**\n","**Task:** Tokenize a given text using both **NLTK** and **spaCy**."]},{"cell_type":"code","execution_count":null,"id":"85a9ac50","metadata":{"id":"85a9ac50","executionInfo":{"status":"ok","timestamp":1741713282550,"user_tz":-210,"elapsed":12208,"user":{"displayName":"abolfazl aghdaee","userId":"17465871458475218528"}},"outputId":"2423787d-ef6a-4249-ab71-b8aac2098150","colab":{"base_uri":"https://localhost:8080/"}},"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n"]},{"output_type":"stream","name":"stdout","text":["NLTK Word Tokens: ['SpaCy', 'is', 'fast', '!', 'However', ',', 'NLTK', 'provides', 'flexibility', 'in', 'tokenization', '.']\n","NLTK Sentence Tokens: ['SpaCy is fast!', 'However, NLTK provides flexibility in tokenization.']\n","spaCy Tokens: ['SpaCy', 'is', 'fast', '!', 'However', ',', 'NLTK', 'provides', 'flexibility', 'in', 'tokenization', '.']\n"]}],"source":["import spacy\n","from nltk.tokenize import word_tokenize, sent_tokenize\n","\n","nltk.download('punkt')\n","nlp = spacy.load('en_core_web_sm')\n","\n","text = \"SpaCy is fast! However, NLTK provides flexibility in tokenization.\"\n","\n","# NLTK Tokenization\n","nltk_word_tokens = word_tokenize(text)  # FILL THIS\n","nltk_sent_tokens = sent_tokenize(text)  # FILL THIS\n","\n","# spaCy Tokenization\n","doc = nlp(text)\n","spacy_tokens = [token.text for token in doc]\n","\n","print(\"NLTK Word Tokens:\", nltk_word_tokens)\n","print(\"NLTK Sentence Tokens:\", nltk_sent_tokens)\n","print(\"spaCy Tokens:\", spacy_tokens)"]},{"cell_type":"markdown","source":["## **3. Regex Pattern Matching for Phone Number Detection**\n","**Task:** Write a pattern using regex to find the phone nymber in the text."],"metadata":{"id":"Sp7RCE4fIaVV"},"id":"Sp7RCE4fIaVV"},{"cell_type":"code","source":["import re\n","\n","# Example 2: Phone Number Extraction\n","text_phones = \"Call me at +1-202-555-0173 or reach our office at (415) 123-4567.\"\n","phone_pattern = r\"\\(?\\d{3}\\)?[-.\\s]?\\d{3}[-.\\s]?\\d{4}\"  # Regex for phone numbers\n","\n","phones = re.findall(phone_pattern, text_phones)\n","print(\"Detected Phone Numbers:\", phones)\n"],"metadata":{"id":"XeSzDZ2-Icdv","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1741713282603,"user_tz":-210,"elapsed":45,"user":{"displayName":"abolfazl aghdaee","userId":"17465871458475218528"}},"outputId":"cb0d63f9-84f3-41d8-9444-9d58dcdf67aa"},"id":"XeSzDZ2-Icdv","execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Detected Phone Numbers: ['202-555-0173', '(415) 123-4567']\n"]}]},{"cell_type":"markdown","source":["## 4. **Stopwords Filtering using NLTK**\n","**Task:** Analyze movie reviews where stopwords are removed to focus on meaningful words."],"metadata":{"id":"pdn94C-1cGgc"},"id":"pdn94C-1cGgc"},{"cell_type":"code","source":["\n","nltk.download(\"stopwords\")\n","nltk.download(\"punkt\")\n","\n","import nltk\n","from nltk.corpus import stopwords\n","from nltk.tokenize import word_tokenize\n","\n","# üé¨ Sample Movie Review\n","review = \"\"\"The movie was absolutely amazing! The cinematography was stunning, and the characters were incredibly well-developed.\n","However, the storyline felt a bit predictable at times, and some scenes were unnecessarily long. Overall, a great experience!\"\"\"\n","\n","# Tokenize words\n","words = word_tokenize(review)\n","\n","# Remove stopwords\n","filtered_words = [word for word in words if word.lower() not in stopwords.words(\"english\") and word.isalnum()]\n","\n","# Output results\n","print(\"Original Words:\", words)\n","print(\"\\nFiltered (No Stopwords):\", filtered_words)\n"],"metadata":{"id":"qFGnnFOjcM23","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1741713282691,"user_tz":-210,"elapsed":86,"user":{"displayName":"abolfazl aghdaee","userId":"17465871458475218528"}},"outputId":"1ca1bf33-09fb-49c6-a8ef-1e0a53c67195"},"id":"qFGnnFOjcM23","execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Original Words: ['The', 'movie', 'was', 'absolutely', 'amazing', '!', 'The', 'cinematography', 'was', 'stunning', ',', 'and', 'the', 'characters', 'were', 'incredibly', 'well-developed', '.', 'However', ',', 'the', 'storyline', 'felt', 'a', 'bit', 'predictable', 'at', 'times', ',', 'and', 'some', 'scenes', 'were', 'unnecessarily', 'long', '.', 'Overall', ',', 'a', 'great', 'experience', '!']\n","\n","Filtered (No Stopwords): ['movie', 'absolutely', 'amazing', 'cinematography', 'stunning', 'characters', 'incredibly', 'However', 'storyline', 'felt', 'bit', 'predictable', 'times', 'scenes', 'unnecessarily', 'long', 'Overall', 'great', 'experience']\n"]},{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n","[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n"]}]},{"cell_type":"markdown","source":["## 5. **Stemming Methods using NLTK**\n","**Task:** Analyze legal and scientific terms to observe how different stemming algorithms behave."],"metadata":{"id":"7UezErpHcu3x"},"id":"7UezErpHcu3x"},{"cell_type":"code","source":["from nltk.stem import PorterStemmer, LancasterStemmer\n","\n","# ‚öñÔ∏è Sample Legal & Scientific Terms\n","words = [\"arguing\", \"justification\", \"liable\", \"obligations\", \"classification\", \"microbiology\", \"evolutionary\", \"running\", \"happiness\"]\n","\n","# Initialize Stemmer Objects\n","porter = PorterStemmer()\n","lancaster = LancasterStemmer()\n","\n","# Apply Stemming\n","porter_stems = [porter.stem(word) for word in words]\n","lancaster_stems = [lancaster.stem(word) for word in words]\n","\n","# Output Results\n","print(\"Original Words:\", words)\n","print(\"\\nPorter Stemmer Results:\", porter_stems)\n","print(\"\\nLancaster Stemmer Results:\", lancaster_stems)\n"],"metadata":{"id":"Kpvzkd31d6Na","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1741713282750,"user_tz":-210,"elapsed":55,"user":{"displayName":"abolfazl aghdaee","userId":"17465871458475218528"}},"outputId":"606d133c-3880-4b9f-ff9e-2f377a7b39bf"},"id":"Kpvzkd31d6Na","execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Original Words: ['arguing', 'justification', 'liable', 'obligations', 'classification', 'microbiology', 'evolutionary', 'running', 'happiness']\n","\n","Porter Stemmer Results: ['argu', 'justif', 'liabl', 'oblig', 'classif', 'microbiolog', 'evolutionari', 'run', 'happi']\n","\n","Lancaster Stemmer Results: ['argu', 'just', 'liabl', 'oblig', 'class', 'microbiolog', 'evolv', 'run', 'happy']\n"]}]},{"cell_type":"markdown","source":["## 6. **Lemmatization Strategies using NLTK & spaCy**\n","\n","### NLTK‚Äôs WordNetLemmatizer\n","**Task:** Lemmatize a political news headline to show how lemmatization helps retain the correct part of speech (POS) while normalizing words."],"metadata":{"id":"ffIhEdRJeoI4"},"id":"ffIhEdRJeoI4"},{"cell_type":"code","source":["nltk.download(\"wordnet\")\n","nltk.download(\"punkt\")\n","\n","import nltk\n","from nltk.stem import WordNetLemmatizer\n","from nltk.corpus import wordnet\n","from nltk.tokenize import word_tokenize\n","\n","# üì∞ Sample News Headline\n","headline = \"The senators debated the increasing regulations affecting technology companies.\"\n","\n","# Tokenize words\n","words = word_tokenize(headline)\n","\n","# Initialize Lemmatizer\n","lemmatizer = WordNetLemmatizer()\n","\n","# Apply Lemmatization (default without POS tagging)\n","lemmatized_words = [lemmatizer.lemmatize(word) for word in words]\n","\n","print(\"Original Words:\", words)\n","print(\"\\nLemmatized Words:\", lemmatized_words)\n"],"metadata":{"id":"o4LlZqSxe086","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1741713288481,"user_tz":-210,"elapsed":5733,"user":{"displayName":"abolfazl aghdaee","userId":"17465871458475218528"}},"outputId":"879a9df0-8b07-48e4-8400-6f6571c63446"},"id":"o4LlZqSxe086","execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n"]},{"output_type":"stream","name":"stdout","text":["Original Words: ['The', 'senators', 'debated', 'the', 'increasing', 'regulations', 'affecting', 'technology', 'companies', '.']\n","\n","Lemmatized Words: ['The', 'senator', 'debated', 'the', 'increasing', 'regulation', 'affecting', 'technology', 'company', '.']\n"]}]},{"cell_type":"markdown","source":["### spaCy‚Äôs Built-in Lemmatizer"],"metadata":{"id":"C61P_pdvfJlH"},"id":"C61P_pdvfJlH"},{"cell_type":"code","source":["import spacy\n","\n","nlp = spacy.load(\"en_core_web_sm\")\n","\n","# Process the same headline\n","doc = nlp(headline)\n","\n","# Apply Lemmatization\n","spacy_lemmatized = [token.lemma_ for token in doc]\n","\n","print(\"\\nspaCy Lemmatized Words:\", spacy_lemmatized)\n"],"metadata":{"id":"4qPNWbqafWt3","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1741713289056,"user_tz":-210,"elapsed":578,"user":{"displayName":"abolfazl aghdaee","userId":"17465871458475218528"}},"outputId":"52c06dc7-5b55-43a8-c609-823e3cb15a9b"},"id":"4qPNWbqafWt3","execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","spaCy Lemmatized Words: ['the', 'senator', 'debate', 'the', 'increase', 'regulation', 'affect', 'technology', 'company', '.']\n"]}]},{"cell_type":"markdown","source":["## 7. **Parsing & Chunking using NLTK**\n","\n","**Task:** Analyze legal contracts and job descriptions where parsing and chunking help extract meaningful phrases like noun phrases (NPs) or verb phrases (VPs)."],"metadata":{"id":"nf-YB5yjfdl5"},"id":"nf-YB5yjfdl5"},{"cell_type":"code","source":["# üìú Task: Extracting Key Phrases from Legal & Job Documents\n","import nltk\n","\n","nltk.download(\"punkt\")\n","nltk.download(\"averaged_perceptron_tagger_eng\")\n","\n","# üìú Sample Legal Contract Text\n","contract_text = \"The tenant shall pay the monthly rent before the 5th of each month.\"\n","\n","# Tokenize & POS Tagging\n","words = nltk.word_tokenize(contract_text)\n","pos_tags = nltk.pos_tag(words)\n","\n","# Define a Chunking Grammar for Noun Phrases (NP)\n","grammar = r\"NP: {<DT>?<JJ>*<NN>+}\"\n","\n","# Apply Chunking\n","chunk_parser = nltk.RegexpParser(grammar)\n","tree = chunk_parser.parse(pos_tags)\n","\n","# Display Results\n","print(\"Chunked Tree:\")\n","tree.pretty_print()\n"],"metadata":{"id":"t9e4fOIuft4n","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1741713615789,"user_tz":-210,"elapsed":654,"user":{"displayName":"abolfazl aghdaee","userId":"17465871458475218528"}},"outputId":"fa1fe015-36a2-4ded-8138-09b9f9a56b0e"},"id":"t9e4fOIuft4n","execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n","[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n","[nltk_data]     /root/nltk_data...\n","[nltk_data]   Unzipping taggers/averaged_perceptron_tagger_eng.zip.\n"]},{"output_type":"stream","name":"stdout","text":["Chunked Tree:\n","                                               S                                                                     \n","    ___________________________________________|__________________________________________________________            \n","   |       |        |       |      |      |    |          NP                      NP                      NP         \n","   |       |        |       |      |      |    |     _____|______         ________|_________         _____|_____      \n","shall/MD pay/VB before/IN the/DT 5th/CD of/IN ./. The/DT     tenant/NN the/DT monthly/JJ rent/NN each/DT     month/NN\n","\n"]}]},{"cell_type":"markdown","source":["## 8. **Exploring Hyponyms & Hypernyms using WordNet (NLTK)**\n","\n","**Task:** Hyponyms (specific terms) and hypernyms (general terms) in scientific and business domains, where hierarchical relationships between words are essential."],"metadata":{"id":"GTmtGRjofqpB"},"id":"GTmtGRjofqpB"},{"cell_type":"code","source":["# üîç Task: Explore Word Relationships in Science & Business\n","from nltk.corpus import wordnet\n","\n","# ü¶Å Find Hypernyms & Hyponyms for \"lion\"\n","word = \"lion\"\n","synset = wordnet.synsets(word)[0]  # Selecting the first synset\n","\n","# Hypernyms (More General Category)\n","hypernyms = synset.hypernyms()\n","print(f\"Hypernyms (More General Concept) of '{word}':\")\n","print([hypernym.name().split('.')[0] for hypernym in hypernyms])\n","\n","# Hyponyms (More Specific Types)\n","hyponyms = synset.hyponyms()\n","print(f\"\\nHyponyms (More Specific Types) of '{word}':\")\n","print([hyponym.name().split('.')[0] for hyponym in hyponyms])\n"],"metadata":{"id":"EELbOY0dgjsK","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1741713990600,"user_tz":-210,"elapsed":13,"user":{"displayName":"abolfazl aghdaee","userId":"17465871458475218528"}},"outputId":"1a1fc77a-366e-4b98-c6a7-dcf750a4379b"},"id":"EELbOY0dgjsK","execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Hypernyms (More General Concept) of 'lion':\n","['big_cat']\n","\n","Hyponyms (More Specific Types) of 'lion':\n","['lionet', 'lion_cub', 'lioness']\n"]}]},{"cell_type":"markdown","id":"fdb5a06b","metadata":{"id":"fdb5a06b"},"source":["## **9. Named Entity Recognition (NER) with spaCy**\n","**Task:** Extract named entities from a complex sentence."]},{"cell_type":"code","execution_count":null,"id":"dc7654be","metadata":{"id":"dc7654be","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1741714006586,"user_tz":-210,"elapsed":2688,"user":{"displayName":"abolfazl aghdaee","userId":"17465871458475218528"}},"outputId":"fff8e7d3-375a-4c8c-b1bc-a6283f06f1c8"},"outputs":[{"output_type":"stream","name":"stdout","text":["Named Entities:\n","1969 -> DATE\n","Neil Armstrong -> PERSON\n","first -> ORDINAL\n","Moon -> PERSON\n","Apollo 11 -> LAW\n"]}],"source":["nlp = spacy.load(\"en_core_web_sm\")\n","text = \"In 1969, Neil Armstrong became the first person to walk on the Moon during the Apollo 11 mission.\"\n","\n","doc = nlp(text)  # FILL THIS\n","\n","print(\"Named Entities:\")\n","for ent in doc.ents:\n","    print(f\"{ent.text} -> {ent.label_}\")"]},{"cell_type":"markdown","source":["*End!*"],"metadata":{"id":"SrL2zsrmPq3p"},"id":"SrL2zsrmPq3p"}],"metadata":{"colab":{"provenance":[{"file_id":"1VfxcDCFdC-9XO-4AOz3j46WVZ0IarAiV","timestamp":1741459431861}]},"language_info":{"name":"python"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"nbformat":4,"nbformat_minor":5}